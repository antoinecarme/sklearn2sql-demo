
One can consider this small report as the sklearn2sql feeedback on the database ecosystem behavior when it comes to machine-generated SQL code for machine learning objects deployment. If you are a database vendor/developer and are interested in improving your database for in-databaase modeling, this can be a starting point.

Please note that here , we use a database to predict machine learning models outputs, like a specialized CPU. the same way a deep learning model can be trained or predicted with a GPU or a FPGA.

These notes are based on sklearn2sql tests/benchamrks realized on the following databases : db2, firebird, hive, impala, monetdb, MS SQL Server, mysql, oracle, pgsql, sqlite and teradata

## Common Table Expressions (CTEs)

A must. All the SQL code generated by sklearn2sl is baed on CTEs. CTEs are kind of very handy table expressions and are seen as tables. CTE usage is a recommended way of generating useful SQL. SQL written this way is clear and can be debugged easily. The structure of th SQL code matches the underlying machine learning model. Using CTEs also helps the SQL optimizer. Databases that do not completely support CTEs are not worth working on ;)

## Floating Point Types / Computations

sklearn2sql uses float64 doubles for all its computations (scores, class probabilties, etc). It would be nice if all databases shared the same IEEE-754 standard for DOUBLE PRECISION types (one can dream a little bit;). This is not the case and a lot of overflow/underflow and precision issues are observed when executing the SQL code for such or such database. Teradata has 13 decimal points supported. | exp, log, softmax, naive-bayes etc

## NaN/Infinity behavior
Oracle does a very strange job here !!!! Other databases, not always better. We use MAX_DOUBLE to represent +Infitnity ...

## Wide Tables

Not all databases have the same behavior when it comes to the maximum number of columns in a table or a CTE. Around 1000 is a common value. | one hot encoding can generate a separate column for each different value in the training dataset columns. Not very useful limitations nowadays. 

Database vendors : Please get rid of this or add it as an option (user-controllable).

## SQL Statement Complexity

A lot of databases do not allow using more than N (64 for sqlite ??) tables/froms/selectables/CTEs in the same select statement.

Database vendors : SQL Complexity errors are not always justified. If you really want to set a limitation, add it as an option.

## SQL Expression Complexity

Very complex epressions (using more than a number of columns in the same expression) are not always allowed. MS SQL server does not allow very complex case when expressions (depth limited to 10). expression depth in sqlite.

Database vendors : SQL Complexity errors are not always justified as not all databases have this issue. Some R&D may be helpful. Also erro messages can be raised only when the system resources are missing (CPU/disk/memory), a depth limit of 10 is exaggerated when one has gigagbytes of memory and a lot of CPU power. If you really want to set the max depth to 10, add it as an option.

## Math Functions

Not all databases have tanh etc, density functions etc

https://en.wikibooks.org/wiki/SQL_Dialects_Reference/Functions_and_expressions/Math_functions/Trigonometric_functions

https://stackoverflow.com/questions/38939718/how-to-calculate-tanh-in-sql-server-2012

## Temporary Tables

sklearn2sql sometimes generates temporary tables to precompute very complex expressions (as a workaround for complexity issues).
Their behavior is not always consistant between databases (global or not, session-based, storage : memory-based  etc)

## Recursive CTEs

Not always supported. Used in recurrent neural networks (LSTM, GRU, RNN)

https://github.com/antoinecarme/keras2sql/issues/2

## Table/column Names Limits

sklearn2sql internally generates partially random temporary tables names and tries to avoid 

When sklearn2sql development started, Oracle did not support table names with more than 30 chars. Seems to be correcetd in oracle 18. GOOD news.

## Identifier Name Case Sensitivity

Upper case of lower case forced by the database can be a source of lookup errors.

Database vendors : Naming objects is a user freedom and is a recommneded way of reflecting object semantics. Please get rid of this too 1900's feature !!!!

## Unicode Support

In our tests, 5 out of 10 database dialects did not support unicode in the table/column/index/key names out of the box.

Using UTF8 by default for a database does not cost too much. Unicode is quite old and stable now (more than 25 years old). 

A database should support all types fo data. Tesxtual data are the most used. Their support is not optional. A lot of C++, Java, ..., and Python  code has been written, to override such or such database bug/behavior when it deals with unicode. Text is TEXT.

Database vendors : Please , please, make utf8 the default encoding for your database. Even in the United States of America ;-).

## Use of Virtual Machines

Hadoop based databases (hive, impala) use a large number of VMs for executing a single SQL statement. The SQL execution is too slow and unacceptable. This makes these databases simply not suitable for this kind of task (SQL-based in-database analytics). The benchmarking process did not always end properly without timeout (thousands of models trained on different table layouts [columns x lines]) 


